{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 551.856506348\n",
      "[step: 1] loss: 402.725799561\n",
      "[step: 2] loss: 278.913604736\n",
      "[step: 3] loss: 182.313873291\n",
      "[step: 4] loss: 117.207847595\n",
      "[step: 5] loss: 84.9883499146\n",
      "[step: 6] loss: 81.1049194336\n",
      "[step: 7] loss: 92.1679153442\n",
      "[step: 8] loss: 96.8047714233\n",
      "[step: 9] loss: 88.0680465698\n",
      "[step: 10] loss: 71.4161148071\n",
      "[step: 11] loss: 53.8768424988\n",
      "[step: 12] loss: 40.0232467651\n",
      "[step: 13] loss: 31.5744132996\n",
      "[step: 14] loss: 28.1536693573\n",
      "[step: 15] loss: 28.2720088959\n",
      "[step: 16] loss: 30.1617546082\n",
      "[step: 17] loss: 32.2988014221\n",
      "[step: 18] loss: 33.6187973022\n",
      "[step: 19] loss: 33.5376205444\n",
      "[step: 20] loss: 31.8935680389\n",
      "[step: 21] loss: 28.8724956512\n",
      "[step: 22] loss: 24.9196300507\n",
      "[step: 23] loss: 20.621963501\n",
      "[step: 24] loss: 16.5641555786\n",
      "[step: 25] loss: 13.1928386688\n",
      "[step: 26] loss: 10.7368345261\n",
      "[step: 27] loss: 9.20631599426\n",
      "[step: 28] loss: 8.45013046265\n",
      "[step: 29] loss: 8.22612476349\n",
      "[step: 30] loss: 8.2575712204\n",
      "[step: 31] loss: 8.28377056122\n",
      "[step: 32] loss: 8.11844444275\n",
      "[step: 33] loss: 7.70085954666\n",
      "[step: 34] loss: 7.10480642319\n",
      "[step: 35] loss: 6.49039554596\n",
      "[step: 36] loss: 6.0208106041\n",
      "[step: 37] loss: 5.78644561768\n",
      "[step: 38] loss: 5.77195835114\n",
      "[step: 39] loss: 5.876932621\n",
      "[step: 40] loss: 5.97263622284\n",
      "[step: 41] loss: 5.96104383469\n",
      "[step: 42] loss: 5.80788421631\n",
      "[step: 43] loss: 5.54218053818\n",
      "[step: 44] loss: 5.23292303085\n",
      "[step: 45] loss: 4.95828294754\n",
      "[step: 46] loss: 4.77798986435\n",
      "[step: 47] loss: 4.71440553665\n",
      "[step: 48] loss: 4.74667930603\n",
      "[step: 49] loss: 4.82094669342\n",
      "[step: 50] loss: 4.87417840958\n",
      "[step: 51] loss: 4.86132335663\n",
      "[step: 52] loss: 4.77210855484\n",
      "[step: 53] loss: 4.62944889069\n",
      "[step: 54] loss: 4.47262334824\n",
      "[step: 55] loss: 4.33683586121\n",
      "[step: 56] loss: 4.24020004272\n",
      "[step: 57] loss: 4.18212747574\n",
      "[step: 58] loss: 4.14970874786\n",
      "[step: 59] loss: 4.12626791\n",
      "[step: 60] loss: 4.09819221497\n",
      "[step: 61] loss: 4.05893230438\n",
      "[step: 62] loss: 4.01031827927\n",
      "[step: 63] loss: 3.96090841293\n",
      "[step: 64] loss: 3.92136573792\n",
      "[step: 65] loss: 3.89858937263\n",
      "[step: 66] loss: 3.89181041718\n",
      "[step: 67] loss: 3.89320874214\n",
      "[step: 68] loss: 3.89247655869\n",
      "[step: 69] loss: 3.88220930099\n",
      "[step: 70] loss: 3.86077618599\n",
      "[step: 71] loss: 3.83167886734\n",
      "[step: 72] loss: 3.80089068413\n",
      "[step: 73] loss: 3.77415847778\n",
      "[step: 74] loss: 3.75517392159\n",
      "[step: 75] loss: 3.74468636513\n",
      "[step: 76] loss: 3.74039459229\n",
      "[step: 77] loss: 3.73790454865\n",
      "[step: 78] loss: 3.73265075684\n",
      "[step: 79] loss: 3.72187995911\n",
      "[step: 80] loss: 3.70564675331\n",
      "[step: 81] loss: 3.68619227409\n",
      "[step: 82] loss: 3.66638946533\n",
      "[step: 83] loss: 3.64833235741\n",
      "[step: 84] loss: 3.6327662468\n",
      "[step: 85] loss: 3.61931085587\n",
      "[step: 86] loss: 3.60700392723\n",
      "[step: 87] loss: 3.59480905533\n",
      "[step: 88] loss: 3.58207011223\n",
      "[step: 89] loss: 3.56876778603\n",
      "[step: 90] loss: 3.5554766655\n",
      "[step: 91] loss: 3.54292988777\n",
      "[step: 92] loss: 3.53146791458\n",
      "[step: 93] loss: 3.52078771591\n",
      "[step: 94] loss: 3.5101788044\n",
      "[step: 95] loss: 3.49901366234\n",
      "[step: 96] loss: 3.48706912994\n",
      "[step: 97] loss: 3.4745824337\n",
      "[step: 98] loss: 3.46201753616\n",
      "[step: 99] loss: 3.44985699654\n",
      "[step: 100] loss: 3.438403368\n",
      "[step: 101] loss: 3.42766308784\n",
      "[step: 102] loss: 3.41733026505\n",
      "[step: 103] loss: 3.40693902969\n",
      "[step: 104] loss: 3.39611673355\n",
      "[step: 105] loss: 3.38476204872\n",
      "[step: 106] loss: 3.37304830551\n",
      "[step: 107] loss: 3.36126112938\n",
      "[step: 108] loss: 3.34961771965\n",
      "[step: 109] loss: 3.33821582794\n",
      "[step: 110] loss: 3.32700943947\n",
      "[step: 111] loss: 3.31589531898\n",
      "[step: 112] loss: 3.30474925041\n",
      "[step: 113] loss: 3.29350280762\n",
      "[step: 114] loss: 3.28218388557\n",
      "[step: 115] loss: 3.27087306976\n",
      "[step: 116] loss: 3.25965046883\n",
      "[step: 117] loss: 3.24852490425\n",
      "[step: 118] loss: 3.23744821548\n",
      "[step: 119] loss: 3.2263507843\n",
      "[step: 120] loss: 3.21519589424\n",
      "[step: 121] loss: 3.20398449898\n",
      "[step: 122] loss: 3.19274997711\n",
      "[step: 123] loss: 3.18154025078\n",
      "[step: 124] loss: 3.17039203644\n",
      "[step: 125] loss: 3.15931296349\n",
      "[step: 126] loss: 3.14827513695\n",
      "[step: 127] loss: 3.13723707199\n",
      "[step: 128] loss: 3.12616658211\n",
      "[step: 129] loss: 3.11505889893\n",
      "[step: 130] loss: 3.103931427\n",
      "[step: 131] loss: 3.09280943871\n",
      "[step: 132] loss: 3.08170747757\n",
      "[step: 133] loss: 3.07062506676\n",
      "[step: 134] loss: 3.05955934525\n",
      "[step: 135] loss: 3.04849433899\n",
      "[step: 136] loss: 3.03742694855\n",
      "[step: 137] loss: 3.02635741234\n",
      "[step: 138] loss: 3.01529836655\n",
      "[step: 139] loss: 3.00426077843\n",
      "[step: 140] loss: 2.99324297905\n",
      "[step: 141] loss: 2.98224401474\n",
      "[step: 142] loss: 2.97125387192\n",
      "[step: 143] loss: 2.96026420593\n",
      "[step: 144] loss: 2.94927763939\n",
      "[step: 145] loss: 2.93829393387\n",
      "[step: 146] loss: 2.92732429504\n",
      "[step: 147] loss: 2.91637134552\n",
      "[step: 148] loss: 2.90543532372\n",
      "[step: 149] loss: 2.89451694489\n",
      "[step: 150] loss: 2.8836081028\n",
      "[step: 151] loss: 2.87271213531\n",
      "[step: 152] loss: 2.86182260513\n",
      "[step: 153] loss: 2.85094618797\n",
      "[step: 154] loss: 2.84008526802\n",
      "[step: 155] loss: 2.82923698425\n",
      "[step: 156] loss: 2.81840920448\n",
      "[step: 157] loss: 2.80759692192\n",
      "[step: 158] loss: 2.79679775238\n",
      "[step: 159] loss: 2.78601312637\n",
      "[step: 160] loss: 2.77524471283\n",
      "[step: 161] loss: 2.76449370384\n",
      "[step: 162] loss: 2.75376200676\n",
      "[step: 163] loss: 2.7430460453\n",
      "[step: 164] loss: 2.73234891891\n",
      "[step: 165] loss: 2.72167134285\n",
      "[step: 166] loss: 2.7110106945\n",
      "[step: 167] loss: 2.70037007332\n",
      "[step: 168] loss: 2.68974876404\n",
      "[step: 169] loss: 2.67914819717\n",
      "[step: 170] loss: 2.66856718063\n",
      "[step: 171] loss: 2.65801119804\n",
      "[step: 172] loss: 2.64747309685\n",
      "[step: 173] loss: 2.63695597649\n",
      "[step: 174] loss: 2.62645959854\n",
      "[step: 175] loss: 2.61598372459\n",
      "[step: 176] loss: 2.6055328846\n",
      "[step: 177] loss: 2.59510374069\n",
      "[step: 178] loss: 2.58469724655\n",
      "[step: 179] loss: 2.57431387901\n",
      "[step: 180] loss: 2.56395864487\n",
      "[step: 181] loss: 2.5536236763\n",
      "[step: 182] loss: 2.54331445694\n",
      "[step: 183] loss: 2.53303074837\n",
      "[step: 184] loss: 2.52277231216\n",
      "[step: 185] loss: 2.51254177094\n",
      "[step: 186] loss: 2.50233674049\n",
      "[step: 187] loss: 2.49215769768\n",
      "[step: 188] loss: 2.482006073\n",
      "[step: 189] loss: 2.47188091278\n",
      "[step: 190] loss: 2.4617857933\n",
      "[step: 191] loss: 2.45171785355\n",
      "[step: 192] loss: 2.44167971611\n",
      "[step: 193] loss: 2.43166923523\n",
      "[step: 194] loss: 2.42169141769\n",
      "[step: 195] loss: 2.41174077988\n",
      "[step: 196] loss: 2.40182113647\n",
      "[step: 197] loss: 2.39193320274\n",
      "[step: 198] loss: 2.38207650185\n",
      "[step: 199] loss: 2.37225079536\n",
      "[step: 200] loss: 2.36245894432\n",
      "[step: 201] loss: 2.35269832611\n",
      "[step: 202] loss: 2.34296870232\n",
      "[step: 203] loss: 2.33327412605\n",
      "[step: 204] loss: 2.32361268997\n",
      "[step: 205] loss: 2.31398630142\n",
      "[step: 206] loss: 2.30439543724\n",
      "[step: 207] loss: 2.2948372364\n",
      "[step: 208] loss: 2.28531336784\n",
      "[step: 209] loss: 2.27582764626\n",
      "[step: 210] loss: 2.26637578011\n",
      "[step: 211] loss: 2.256960392\n",
      "[step: 212] loss: 2.24758315086\n",
      "[step: 213] loss: 2.23824357986\n",
      "[step: 214] loss: 2.22894024849\n",
      "[step: 215] loss: 2.21967387199\n",
      "[step: 216] loss: 2.21044683456\n",
      "[step: 217] loss: 2.20125865936\n",
      "[step: 218] loss: 2.19210839272\n",
      "[step: 219] loss: 2.1829996109\n",
      "[step: 220] loss: 2.17392921448\n",
      "[step: 221] loss: 2.16489911079\n",
      "[step: 222] loss: 2.15590882301\n",
      "[step: 223] loss: 2.1469604969\n",
      "[step: 224] loss: 2.13805246353\n",
      "[step: 225] loss: 2.1291847229\n",
      "[step: 226] loss: 2.12036108971\n",
      "[step: 227] loss: 2.11157846451\n",
      "[step: 228] loss: 2.10283780098\n",
      "[step: 229] loss: 2.09413981438\n",
      "[step: 230] loss: 2.08548593521\n",
      "[step: 231] loss: 2.07687473297\n",
      "[step: 232] loss: 2.06830573082\n",
      "[step: 233] loss: 2.05978345871\n",
      "[step: 234] loss: 2.05130457878\n",
      "[step: 235] loss: 2.04287004471\n",
      "[step: 236] loss: 2.03447985649\n",
      "[step: 237] loss: 2.02613353729\n",
      "[step: 238] loss: 2.01783585548\n",
      "[step: 239] loss: 2.00958180428\n",
      "[step: 240] loss: 2.00137329102\n",
      "[step: 241] loss: 1.99321103096\n",
      "[step: 242] loss: 1.98509609699\n",
      "[step: 243] loss: 1.97702634335\n",
      "[step: 244] loss: 1.96900451183\n",
      "[step: 245] loss: 1.96103000641\n",
      "[step: 246] loss: 1.95310258865\n",
      "[step: 247] loss: 1.94522213936\n",
      "[step: 248] loss: 1.93738746643\n",
      "[step: 249] loss: 1.92960309982\n",
      "[step: 250] loss: 1.92186689377\n",
      "[step: 251] loss: 1.91417765617\n",
      "[step: 252] loss: 1.90653824806\n",
      "[step: 253] loss: 1.89894533157\n",
      "[step: 254] loss: 1.89140272141\n",
      "[step: 255] loss: 1.88390731812\n",
      "[step: 256] loss: 1.87646138668\n",
      "[step: 257] loss: 1.8690649271\n",
      "[step: 258] loss: 1.86171770096\n",
      "[step: 259] loss: 1.85441923141\n",
      "[step: 260] loss: 1.84717047215\n",
      "[step: 261] loss: 1.83997070789\n",
      "[step: 262] loss: 1.83282053471\n",
      "[step: 263] loss: 1.82572054863\n",
      "[step: 264] loss: 1.81866979599\n",
      "[step: 265] loss: 1.81166851521\n",
      "[step: 266] loss: 1.80471742153\n",
      "[step: 267] loss: 1.79781496525\n",
      "[step: 268] loss: 1.7909642458\n",
      "[step: 269] loss: 1.78416097164\n",
      "[step: 270] loss: 1.77740955353\n",
      "[step: 271] loss: 1.77070701122\n",
      "[step: 272] loss: 1.76405405998\n",
      "[step: 273] loss: 1.7574505806\n",
      "[step: 274] loss: 1.750898242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 275] loss: 1.74439406395\n",
      "[step: 276] loss: 1.73793935776\n",
      "[step: 277] loss: 1.73153603077\n",
      "[step: 278] loss: 1.72517943382\n",
      "[step: 279] loss: 1.71887457371\n",
      "[step: 280] loss: 1.71261715889\n",
      "[step: 281] loss: 1.70640969276\n",
      "[step: 282] loss: 1.70025193691\n",
      "[step: 283] loss: 1.69414186478\n",
      "[step: 284] loss: 1.68808197975\n",
      "[step: 285] loss: 1.68206882477\n",
      "[step: 286] loss: 1.67610633373\n",
      "[step: 287] loss: 1.67019128799\n",
      "[step: 288] loss: 1.66432368755\n",
      "[step: 289] loss: 1.65850448608\n",
      "[step: 290] loss: 1.65273332596\n",
      "[step: 291] loss: 1.64701008797\n",
      "[step: 292] loss: 1.64133310318\n",
      "[step: 293] loss: 1.63570380211\n",
      "[step: 294] loss: 1.63012135029\n",
      "[step: 295] loss: 1.62458598614\n",
      "[step: 296] loss: 1.61909687519\n",
      "[step: 297] loss: 1.61365377903\n",
      "[step: 298] loss: 1.60825705528\n",
      "[step: 299] loss: 1.60290491581\n",
      "[step: 300] loss: 1.59759879112\n",
      "[step: 301] loss: 1.59233808517\n",
      "[step: 302] loss: 1.58712100983\n",
      "[step: 303] loss: 1.58195030689\n",
      "[step: 304] loss: 1.57682287693\n",
      "[step: 305] loss: 1.57173860073\n",
      "[step: 306] loss: 1.56669890881\n",
      "[step: 307] loss: 1.56170237064\n",
      "[step: 308] loss: 1.55674803257\n",
      "[step: 309] loss: 1.55183649063\n",
      "[step: 310] loss: 1.54696750641\n",
      "[step: 311] loss: 1.54214036465\n",
      "[step: 312] loss: 1.537353158\n",
      "[step: 313] loss: 1.53260827065\n",
      "[step: 314] loss: 1.52790534496\n",
      "[step: 315] loss: 1.52324044704\n",
      "[step: 316] loss: 1.5186175108\n",
      "[step: 317] loss: 1.51403295994\n",
      "[step: 318] loss: 1.50948822498\n",
      "[step: 319] loss: 1.50498211384\n",
      "[step: 320] loss: 1.50051391125\n",
      "[step: 321] loss: 1.49608385563\n",
      "[step: 322] loss: 1.49169242382\n",
      "[step: 323] loss: 1.48733794689\n",
      "[step: 324] loss: 1.48301923275\n",
      "[step: 325] loss: 1.47873747349\n",
      "[step: 326] loss: 1.47449195385\n",
      "[step: 327] loss: 1.47028124332\n",
      "[step: 328] loss: 1.46610617638\n",
      "[step: 329] loss: 1.46196591854\n",
      "[step: 330] loss: 1.45786046982\n",
      "[step: 331] loss: 1.4537872076\n",
      "[step: 332] loss: 1.44974911213\n",
      "[step: 333] loss: 1.44574296474\n",
      "[step: 334] loss: 1.44177031517\n",
      "[step: 335] loss: 1.4378298521\n",
      "[step: 336] loss: 1.43392014503\n",
      "[step: 337] loss: 1.4300429821\n",
      "[step: 338] loss: 1.42619621754\n",
      "[step: 339] loss: 1.42237961292\n",
      "[step: 340] loss: 1.41859376431\n",
      "[step: 341] loss: 1.41483712196\n",
      "[step: 342] loss: 1.41111040115\n",
      "[step: 343] loss: 1.40741205215\n",
      "[step: 344] loss: 1.40374267101\n",
      "[step: 345] loss: 1.4001005888\n",
      "[step: 346] loss: 1.39648795128\n",
      "[step: 347] loss: 1.39289975166\n",
      "[step: 348] loss: 1.38934099674\n",
      "[step: 349] loss: 1.38580691814\n",
      "[step: 350] loss: 1.38229978085\n",
      "[step: 351] loss: 1.37881827354\n",
      "[step: 352] loss: 1.37536180019\n",
      "[step: 353] loss: 1.37193083763\n",
      "[step: 354] loss: 1.36852395535\n",
      "[step: 355] loss: 1.36514186859\n",
      "[step: 356] loss: 1.36178302765\n",
      "[step: 357] loss: 1.35844755173\n",
      "[step: 358] loss: 1.35513663292\n",
      "[step: 359] loss: 1.35184645653\n",
      "[step: 360] loss: 1.34857988358\n",
      "[step: 361] loss: 1.34533441067\n",
      "[step: 362] loss: 1.34211170673\n",
      "[step: 363] loss: 1.33891010284\n",
      "[step: 364] loss: 1.33572924137\n",
      "[step: 365] loss: 1.33257007599\n",
      "[step: 366] loss: 1.32942950726\n",
      "[step: 367] loss: 1.32631075382\n",
      "[step: 368] loss: 1.32321119308\n",
      "[step: 369] loss: 1.32013070583\n",
      "[step: 370] loss: 1.31706976891\n",
      "[step: 371] loss: 1.3140283823\n",
      "[step: 372] loss: 1.31100451946\n",
      "[step: 373] loss: 1.30799901485\n",
      "[step: 374] loss: 1.30501234531\n",
      "[step: 375] loss: 1.30204355717\n",
      "[step: 376] loss: 1.29909241199\n",
      "[step: 377] loss: 1.29615664482\n",
      "[step: 378] loss: 1.29323899746\n",
      "[step: 379] loss: 1.29033851624\n",
      "[step: 380] loss: 1.28745496273\n",
      "[step: 381] loss: 1.28458607197\n",
      "[step: 382] loss: 1.28173387051\n",
      "[step: 383] loss: 1.27889764309\n",
      "[step: 384] loss: 1.2760771513\n",
      "[step: 385] loss: 1.27327108383\n",
      "[step: 386] loss: 1.27048122883\n",
      "[step: 387] loss: 1.26770567894\n",
      "[step: 388] loss: 1.26494467258\n",
      "[step: 389] loss: 1.26219797134\n",
      "[step: 390] loss: 1.25946581364\n",
      "[step: 391] loss: 1.25674772263\n",
      "[step: 392] loss: 1.25404298306\n",
      "[step: 393] loss: 1.25135219097\n",
      "[step: 394] loss: 1.24867510796\n",
      "[step: 395] loss: 1.24601125717\n",
      "[step: 396] loss: 1.24336028099\n",
      "[step: 397] loss: 1.24072265625\n",
      "[step: 398] loss: 1.23809742928\n",
      "[step: 399] loss: 1.23548483849\n",
      "[step: 400] loss: 1.2328851223\n",
      "[step: 401] loss: 1.23029696941\n",
      "[step: 402] loss: 1.22772121429\n",
      "[step: 403] loss: 1.22515809536\n",
      "[step: 404] loss: 1.22260570526\n",
      "[step: 405] loss: 1.2200666666\n",
      "[step: 406] loss: 1.21753752232\n",
      "[step: 407] loss: 1.21501994133\n",
      "[step: 408] loss: 1.21251416206\n",
      "[step: 409] loss: 1.21001982689\n",
      "[step: 410] loss: 1.20753633976\n",
      "[step: 411] loss: 1.20506298542\n",
      "[step: 412] loss: 1.20260107517\n",
      "[step: 413] loss: 1.20015025139\n",
      "[step: 414] loss: 1.19770884514\n",
      "[step: 415] loss: 1.19527876377\n",
      "[step: 416] loss: 1.19285869598\n",
      "[step: 417] loss: 1.19044864178\n",
      "[step: 418] loss: 1.18804979324\n",
      "[step: 419] loss: 1.18565928936\n",
      "[step: 420] loss: 1.18327975273\n",
      "[step: 421] loss: 1.18090987206\n",
      "[step: 422] loss: 1.17854893208\n",
      "[step: 423] loss: 1.1761983633\n",
      "[step: 424] loss: 1.17385697365\n",
      "[step: 425] loss: 1.17152512074\n",
      "[step: 426] loss: 1.16920280457\n",
      "[step: 427] loss: 1.16688907146\n",
      "[step: 428] loss: 1.16458511353\n",
      "[step: 429] loss: 1.16229057312\n",
      "[step: 430] loss: 1.16000497341\n",
      "[step: 431] loss: 1.15772747993\n",
      "[step: 432] loss: 1.15545904636\n",
      "[step: 433] loss: 1.15319907665\n",
      "[step: 434] loss: 1.15094828606\n",
      "[step: 435] loss: 1.14870595932\n",
      "[step: 436] loss: 1.1464728117\n",
      "[step: 437] loss: 1.14424741268\n",
      "[step: 438] loss: 1.14203023911\n",
      "[step: 439] loss: 1.13982176781\n",
      "[step: 440] loss: 1.13762152195\n",
      "[step: 441] loss: 1.13542973995\n",
      "[step: 442] loss: 1.13324546814\n",
      "[step: 443] loss: 1.13106918335\n",
      "[step: 444] loss: 1.12890124321\n",
      "[step: 445] loss: 1.12674081326\n",
      "[step: 446] loss: 1.12458944321\n",
      "[step: 447] loss: 1.12244451046\n",
      "[step: 448] loss: 1.12030827999\n",
      "[step: 449] loss: 1.11817991734\n",
      "[step: 450] loss: 1.1160582304\n",
      "[step: 451] loss: 1.1139446497\n",
      "[step: 452] loss: 1.11183953285\n",
      "[step: 453] loss: 1.10973966122\n",
      "[step: 454] loss: 1.10764861107\n",
      "[step: 455] loss: 1.10556447506\n",
      "[step: 456] loss: 1.10348796844\n",
      "[step: 457] loss: 1.1014187336\n",
      "[step: 458] loss: 1.09935760498\n",
      "[step: 459] loss: 1.0973020792\n",
      "[step: 460] loss: 1.09525430202\n",
      "[step: 461] loss: 1.0932135582\n",
      "[step: 462] loss: 1.09118044376\n",
      "[step: 463] loss: 1.08915364742\n",
      "[step: 464] loss: 1.08713388443\n",
      "[step: 465] loss: 1.08512091637\n",
      "[step: 466] loss: 1.08311522007\n",
      "[step: 467] loss: 1.08111655712\n",
      "[step: 468] loss: 1.07912409306\n",
      "[step: 469] loss: 1.07713770866\n",
      "[step: 470] loss: 1.07515883446\n",
      "[step: 471] loss: 1.07318627834\n",
      "[step: 472] loss: 1.07122063637\n",
      "[step: 473] loss: 1.0692615509\n",
      "[step: 474] loss: 1.06730926037\n",
      "[step: 475] loss: 1.0653629303\n",
      "[step: 476] loss: 1.06342291832\n",
      "[step: 477] loss: 1.06148946285\n",
      "[step: 478] loss: 1.05956268311\n",
      "[step: 479] loss: 1.0576416254\n",
      "[step: 480] loss: 1.05572724342\n",
      "[step: 481] loss: 1.05381894112\n",
      "[step: 482] loss: 1.05191671848\n",
      "[step: 483] loss: 1.05002081394\n",
      "[step: 484] loss: 1.04813146591\n",
      "[step: 485] loss: 1.04624772072\n",
      "[step: 486] loss: 1.04437017441\n",
      "[step: 487] loss: 1.04249894619\n",
      "[step: 488] loss: 1.04063355923\n",
      "[step: 489] loss: 1.03877365589\n",
      "[step: 490] loss: 1.03692007065\n",
      "[step: 491] loss: 1.03507232666\n",
      "[step: 492] loss: 1.0332300663\n",
      "[step: 493] loss: 1.03139412403\n",
      "[step: 494] loss: 1.02956414223\n",
      "[step: 495] loss: 1.02773940563\n",
      "[step: 496] loss: 1.0259206295\n",
      "[step: 497] loss: 1.024107337\n",
      "[step: 498] loss: 1.02229988575\n",
      "[step: 499] loss: 1.02049815655\n",
      "[step: 500] loss: 1.01870155334\n",
      "[step: 501] loss: 1.01691067219\n",
      "[step: 502] loss: 1.0151257515\n",
      "[step: 503] loss: 1.01334631443\n",
      "[step: 504] loss: 1.01157224178\n",
      "[step: 505] loss: 1.00980329514\n",
      "[step: 506] loss: 1.00804018974\n",
      "[step: 507] loss: 1.00628244877\n",
      "[step: 508] loss: 1.00453007221\n",
      "[step: 509] loss: 1.00278246403\n",
      "[step: 510] loss: 1.00104045868\n",
      "[step: 511] loss: 0.999303877354\n",
      "[step: 512] loss: 0.997572720051\n",
      "[step: 513] loss: 0.995846271515\n",
      "[step: 514] loss: 0.994125187397\n",
      "[step: 515] loss: 0.992409586906\n",
      "[step: 516] loss: 0.990698993206\n",
      "[step: 517] loss: 0.988992869854\n",
      "[step: 518] loss: 0.987292647362\n",
      "[step: 519] loss: 0.985597550869\n",
      "[step: 520] loss: 0.983906805515\n",
      "[step: 521] loss: 0.982221841812\n",
      "[step: 522] loss: 0.980541586876\n",
      "[step: 523] loss: 0.97886544466\n",
      "[step: 524] loss: 0.97719502449\n",
      "[step: 525] loss: 0.975530028343\n",
      "[step: 526] loss: 0.973869144917\n",
      "[step: 527] loss: 0.972213029861\n",
      "[step: 528] loss: 0.970562338829\n",
      "[step: 529] loss: 0.968916118145\n",
      "[step: 530] loss: 0.96727502346\n",
      "[step: 531] loss: 0.965637624264\n",
      "[step: 532] loss: 0.964005768299\n",
      "[step: 533] loss: 0.962379038334\n",
      "[step: 534] loss: 0.96075630188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 535] loss: 0.959138572216\n",
      "[step: 536] loss: 0.95752543211\n",
      "[step: 537] loss: 0.955916881561\n",
      "[step: 538] loss: 0.954312801361\n",
      "[step: 539] loss: 0.952713429928\n",
      "[step: 540] loss: 0.951118290424\n",
      "[step: 541] loss: 0.949528157711\n",
      "[step: 542] loss: 0.947942793369\n",
      "[step: 543] loss: 0.946361005306\n",
      "[step: 544] loss: 0.944784641266\n",
      "[step: 545] loss: 0.943212270737\n",
      "[step: 546] loss: 0.941644430161\n",
      "[step: 547] loss: 0.940080702305\n",
      "[step: 548] loss: 0.938522040844\n",
      "[step: 549] loss: 0.936966776848\n",
      "[step: 550] loss: 0.935416281223\n",
      "[step: 551] loss: 0.933870315552\n",
      "[step: 552] loss: 0.932329177856\n",
      "[step: 553] loss: 0.93079161644\n",
      "[step: 554] loss: 0.92925798893\n",
      "[step: 555] loss: 0.92772936821\n",
      "[step: 556] loss: 0.926204800606\n",
      "[step: 557] loss: 0.924684345722\n",
      "[step: 558] loss: 0.923168361187\n",
      "[step: 559] loss: 0.92165595293\n",
      "[step: 560] loss: 0.920148313046\n",
      "[step: 561] loss: 0.918644428253\n",
      "[step: 562] loss: 0.917145013809\n",
      "[step: 563] loss: 0.915649533272\n",
      "[step: 564] loss: 0.914158165455\n",
      "[step: 565] loss: 0.912671029568\n",
      "[step: 566] loss: 0.911187827587\n",
      "[step: 567] loss: 0.909708917141\n",
      "[step: 568] loss: 0.90823328495\n",
      "[step: 569] loss: 0.906762242317\n",
      "[step: 570] loss: 0.905295193195\n",
      "[step: 571] loss: 0.903832912445\n",
      "[step: 572] loss: 0.902373194695\n",
      "[step: 573] loss: 0.900918245316\n",
      "[step: 574] loss: 0.899466991425\n",
      "[step: 575] loss: 0.898019194603\n",
      "[step: 576] loss: 0.896576523781\n",
      "[step: 577] loss: 0.895136594772\n",
      "[step: 578] loss: 0.893700838089\n",
      "[step: 579] loss: 0.892268955708\n",
      "[step: 580] loss: 0.890841066837\n",
      "[step: 581] loss: 0.889417052269\n",
      "[step: 582] loss: 0.887997210026\n",
      "[step: 583] loss: 0.886580824852\n",
      "[step: 584] loss: 0.885167956352\n",
      "[step: 585] loss: 0.883759081364\n",
      "[step: 586] loss: 0.882354319096\n",
      "[step: 587] loss: 0.880952715874\n",
      "[step: 588] loss: 0.879555106163\n",
      "[step: 589] loss: 0.878161251545\n",
      "[step: 590] loss: 0.876770734787\n",
      "[step: 591] loss: 0.875384569168\n",
      "[step: 592] loss: 0.874001979828\n",
      "[step: 593] loss: 0.872622907162\n",
      "[step: 594] loss: 0.871246993542\n",
      "[step: 595] loss: 0.869876623154\n",
      "[step: 596] loss: 0.868507802486\n",
      "[step: 597] loss: 0.86714309454\n",
      "[step: 598] loss: 0.865782320499\n",
      "[step: 599] loss: 0.864425778389\n",
      "[step: 600] loss: 0.863071858883\n",
      "[step: 601] loss: 0.861721515656\n",
      "[step: 602] loss: 0.860374748707\n",
      "[step: 603] loss: 0.859032154083\n",
      "[step: 604] loss: 0.857693135738\n",
      "[step: 605] loss: 0.856356918812\n",
      "[step: 606] loss: 0.85502499342\n",
      "[step: 607] loss: 0.853696644306\n",
      "[step: 608] loss: 0.852370977402\n",
      "[step: 609] loss: 0.851049482822\n",
      "[step: 610] loss: 0.849731326103\n",
      "[step: 611] loss: 0.848416149616\n",
      "[step: 612] loss: 0.847105443478\n",
      "[step: 613] loss: 0.845797777176\n",
      "[step: 614] loss: 0.844493031502\n",
      "[step: 615] loss: 0.843192398548\n",
      "[step: 616] loss: 0.841894984245\n",
      "[step: 617] loss: 0.840600907803\n",
      "[step: 618] loss: 0.83931016922\n",
      "[step: 619] loss: 0.838023245335\n",
      "[step: 620] loss: 0.836739182472\n",
      "[step: 621] loss: 0.835458993912\n",
      "[step: 622] loss: 0.834181964397\n",
      "[step: 623] loss: 0.832907855511\n",
      "[step: 624] loss: 0.831637918949\n",
      "[step: 625] loss: 0.830370903015\n",
      "[step: 626] loss: 0.829107165337\n",
      "[step: 627] loss: 0.827847123146\n",
      "[step: 628] loss: 0.826590061188\n",
      "[step: 629] loss: 0.825336396694\n",
      "[step: 630] loss: 0.824085891247\n",
      "[step: 631] loss: 0.822839140892\n",
      "[step: 632] loss: 0.821595728397\n",
      "[step: 633] loss: 0.820355117321\n",
      "[step: 634] loss: 0.819118261337\n",
      "[step: 635] loss: 0.817884385586\n",
      "[step: 636] loss: 0.816653430462\n",
      "[step: 637] loss: 0.815426230431\n",
      "[step: 638] loss: 0.81420224905\n",
      "[step: 639] loss: 0.812981903553\n",
      "[step: 640] loss: 0.811763882637\n",
      "[step: 641] loss: 0.810549497604\n",
      "[step: 642] loss: 0.809337854385\n",
      "[step: 643] loss: 0.808130145073\n",
      "[step: 644] loss: 0.806925177574\n",
      "[step: 645] loss: 0.805723965168\n",
      "[step: 646] loss: 0.804525852203\n",
      "[step: 647] loss: 0.803330242634\n",
      "[step: 648] loss: 0.802138328552\n",
      "[step: 649] loss: 0.800949454308\n",
      "[step: 650] loss: 0.799763977528\n",
      "[step: 651] loss: 0.798581540585\n",
      "[step: 652] loss: 0.797401249409\n",
      "[step: 653] loss: 0.796225368977\n",
      "[step: 654] loss: 0.7950527668\n",
      "[step: 655] loss: 0.793881952763\n",
      "[step: 656] loss: 0.79271531105\n",
      "[step: 657] loss: 0.791551351547\n",
      "[step: 658] loss: 0.790390789509\n",
      "[step: 659] loss: 0.789233028889\n",
      "[step: 660] loss: 0.788078248501\n",
      "[step: 661] loss: 0.786927044392\n",
      "[step: 662] loss: 0.785778343678\n",
      "[step: 663] loss: 0.784633457661\n",
      "[step: 664] loss: 0.783490777016\n",
      "[step: 665] loss: 0.782351493835\n",
      "[step: 666] loss: 0.781215786934\n",
      "[step: 667] loss: 0.780081689358\n",
      "[step: 668] loss: 0.77895206213\n",
      "[step: 669] loss: 0.777824103832\n",
      "[step: 670] loss: 0.77670019865\n",
      "[step: 671] loss: 0.775578975677\n",
      "[step: 672] loss: 0.774460971355\n",
      "[step: 673] loss: 0.773345768452\n",
      "[step: 674] loss: 0.772233009338\n",
      "[step: 675] loss: 0.771124064922\n",
      "[step: 676] loss: 0.770017206669\n",
      "[step: 677] loss: 0.768913984299\n",
      "[step: 678] loss: 0.767813563347\n",
      "[step: 679] loss: 0.766715943813\n",
      "[step: 680] loss: 0.765621542931\n",
      "[step: 681] loss: 0.764529585838\n",
      "[step: 682] loss: 0.763440608978\n",
      "[step: 683] loss: 0.76235473156\n",
      "[step: 684] loss: 0.761272072792\n",
      "[step: 685] loss: 0.760191380978\n",
      "[step: 686] loss: 0.759114205837\n",
      "[step: 687] loss: 0.75803989172\n",
      "[step: 688] loss: 0.756968319416\n",
      "[step: 689] loss: 0.75589966774\n",
      "[step: 690] loss: 0.754833579063\n",
      "[step: 691] loss: 0.753770232201\n",
      "[step: 692] loss: 0.752709746361\n",
      "[step: 693] loss: 0.751652479172\n",
      "[step: 694] loss: 0.750598073006\n",
      "[step: 695] loss: 0.749545991421\n",
      "[step: 696] loss: 0.748497188091\n",
      "[step: 697] loss: 0.747450649738\n",
      "[step: 698] loss: 0.746407091618\n",
      "[step: 699] loss: 0.745366275311\n",
      "[step: 700] loss: 0.744328260422\n",
      "[step: 701] loss: 0.743292987347\n",
      "[step: 702] loss: 0.742259800434\n",
      "[step: 703] loss: 0.74123030901\n",
      "[step: 704] loss: 0.740203440189\n",
      "[step: 705] loss: 0.739178597927\n",
      "[step: 706] loss: 0.738156795502\n",
      "[step: 707] loss: 0.737137258053\n",
      "[step: 708] loss: 0.736121118069\n",
      "[step: 709] loss: 0.735107421875\n",
      "[step: 710] loss: 0.734096229076\n",
      "[step: 711] loss: 0.73308712244\n",
      "[step: 712] loss: 0.732081711292\n",
      "[step: 713] loss: 0.731078326702\n",
      "[step: 714] loss: 0.730077922344\n",
      "[step: 715] loss: 0.729079425335\n",
      "[step: 716] loss: 0.728084027767\n",
      "[step: 717] loss: 0.727091014385\n",
      "[step: 718] loss: 0.726100623608\n",
      "[step: 719] loss: 0.725112318993\n",
      "[step: 720] loss: 0.724127233028\n",
      "[step: 721] loss: 0.723144054413\n",
      "[step: 722] loss: 0.722164392471\n",
      "[step: 723] loss: 0.721186339855\n",
      "[step: 724] loss: 0.72021150589\n",
      "[step: 725] loss: 0.719238460064\n",
      "[step: 726] loss: 0.718268096447\n",
      "[step: 727] loss: 0.717299818993\n",
      "[step: 728] loss: 0.716335237026\n",
      "[step: 729] loss: 0.715371668339\n",
      "[step: 730] loss: 0.714411199093\n",
      "[step: 731] loss: 0.713453173637\n",
      "[step: 732] loss: 0.71249717474\n",
      "[step: 733] loss: 0.711543798447\n",
      "[step: 734] loss: 0.710592627525\n",
      "[step: 735] loss: 0.709644436836\n",
      "[step: 736] loss: 0.708698034286\n",
      "[step: 737] loss: 0.70775449276\n",
      "[step: 738] loss: 0.706812620163\n",
      "[step: 739] loss: 0.705873250961\n",
      "[step: 740] loss: 0.704936444759\n",
      "[step: 741] loss: 0.704001605511\n",
      "[step: 742] loss: 0.703069627285\n",
      "[step: 743] loss: 0.702139496803\n",
      "[step: 744] loss: 0.701211750507\n",
      "[step: 745] loss: 0.700286209583\n",
      "[step: 746] loss: 0.699362933636\n",
      "[step: 747] loss: 0.698441624641\n",
      "[step: 748] loss: 0.697522878647\n",
      "[step: 749] loss: 0.696606218815\n",
      "[step: 750] loss: 0.695691585541\n",
      "[step: 751] loss: 0.694779515266\n",
      "[step: 752] loss: 0.693868935108\n",
      "[step: 753] loss: 0.692961037159\n",
      "[step: 754] loss: 0.692055284977\n",
      "[step: 755] loss: 0.691151678562\n",
      "[step: 756] loss: 0.690249860287\n",
      "[step: 757] loss: 0.689350545406\n",
      "[step: 758] loss: 0.688452422619\n",
      "[step: 759] loss: 0.687557637691\n",
      "[step: 760] loss: 0.686664164066\n",
      "[step: 761] loss: 0.685773193836\n",
      "[step: 762] loss: 0.684883832932\n",
      "[step: 763] loss: 0.683996915817\n",
      "[step: 764] loss: 0.683111429214\n",
      "[step: 765] loss: 0.682228446007\n",
      "[step: 766] loss: 0.681347131729\n",
      "[step: 767] loss: 0.680468082428\n",
      "[step: 768] loss: 0.679590940475\n",
      "[step: 769] loss: 0.678715646267\n",
      "[step: 770] loss: 0.677842378616\n",
      "[step: 771] loss: 0.676971018314\n",
      "[step: 772] loss: 0.676100969315\n",
      "[step: 773] loss: 0.675233840942\n",
      "[step: 774] loss: 0.674368262291\n",
      "[step: 775] loss: 0.673504650593\n",
      "[step: 776] loss: 0.67264264822\n",
      "[step: 777] loss: 0.671782314777\n",
      "[step: 778] loss: 0.67092436552\n",
      "[step: 779] loss: 0.670068025589\n",
      "[step: 780] loss: 0.669213354588\n",
      "[step: 781] loss: 0.668360829353\n",
      "[step: 782] loss: 0.667509377003\n",
      "[step: 783] loss: 0.666660010815\n",
      "[step: 784] loss: 0.665812849998\n",
      "[step: 785] loss: 0.664967417717\n",
      "[step: 786] loss: 0.664123237133\n",
      "[step: 787] loss: 0.663280904293\n",
      "[step: 788] loss: 0.662440657616\n",
      "[step: 789] loss: 0.66160184145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 790] loss: 0.660764873028\n",
      "[step: 791] loss: 0.659929633141\n",
      "[step: 792] loss: 0.65909576416\n",
      "[step: 793] loss: 0.658263802528\n",
      "[step: 794] loss: 0.657433390617\n",
      "[step: 795] loss: 0.656604647636\n",
      "[step: 796] loss: 0.655777812004\n",
      "[step: 797] loss: 0.654951751232\n",
      "[step: 798] loss: 0.654127776623\n",
      "[step: 799] loss: 0.653305828571\n",
      "[step: 800] loss: 0.652485489845\n",
      "[step: 801] loss: 0.651665985584\n",
      "[step: 802] loss: 0.650848209858\n",
      "[step: 803] loss: 0.650032281876\n",
      "[step: 804] loss: 0.649217844009\n",
      "[step: 805] loss: 0.648404955864\n",
      "[step: 806] loss: 0.647593379021\n",
      "[step: 807] loss: 0.646783292294\n",
      "[step: 808] loss: 0.645974576473\n",
      "[step: 809] loss: 0.64516800642\n",
      "[step: 810] loss: 0.644362032413\n",
      "[step: 811] loss: 0.643557965755\n",
      "[step: 812] loss: 0.64275509119\n",
      "[step: 813] loss: 0.641954243183\n",
      "[step: 814] loss: 0.641154289246\n",
      "[step: 815] loss: 0.64035576582\n",
      "[step: 816] loss: 0.639559090137\n",
      "[step: 817] loss: 0.638763844967\n",
      "[step: 818] loss: 0.637969493866\n",
      "[step: 819] loss: 0.637176156044\n",
      "[step: 820] loss: 0.636385202408\n",
      "[step: 821] loss: 0.635594904423\n",
      "[step: 822] loss: 0.634806334972\n",
      "[step: 823] loss: 0.634018778801\n",
      "[step: 824] loss: 0.633232653141\n",
      "[step: 825] loss: 0.632447719574\n",
      "[step: 826] loss: 0.631664156914\n",
      "[step: 827] loss: 0.630881905556\n",
      "[step: 828] loss: 0.630100727081\n",
      "[step: 829] loss: 0.629321694374\n",
      "[step: 830] loss: 0.628542840481\n",
      "[step: 831] loss: 0.627765476704\n",
      "[step: 832] loss: 0.626989901066\n",
      "[step: 833] loss: 0.626215040684\n",
      "[step: 834] loss: 0.625441730022\n",
      "[step: 835] loss: 0.624669551849\n",
      "[step: 836] loss: 0.623898386955\n",
      "[step: 837] loss: 0.623128294945\n",
      "[step: 838] loss: 0.622359752655\n",
      "[step: 839] loss: 0.621592342854\n",
      "[step: 840] loss: 0.620825946331\n",
      "[step: 841] loss: 0.620060622692\n",
      "[step: 842] loss: 0.619296789169\n",
      "[step: 843] loss: 0.618533909321\n",
      "[step: 844] loss: 0.617771923542\n",
      "[step: 845] loss: 0.617011666298\n",
      "[step: 846] loss: 0.616251885891\n",
      "[step: 847] loss: 0.615493714809\n",
      "[step: 848] loss: 0.614736020565\n",
      "[step: 849] loss: 0.613980174065\n",
      "[step: 850] loss: 0.613225281239\n",
      "[step: 851] loss: 0.612471163273\n",
      "[step: 852] loss: 0.611717760563\n",
      "[step: 853] loss: 0.61096560955\n",
      "[step: 854] loss: 0.610215246677\n",
      "[step: 855] loss: 0.609465718269\n",
      "[step: 856] loss: 0.608716309071\n",
      "[step: 857] loss: 0.607968509197\n",
      "[step: 858] loss: 0.607221961021\n",
      "[step: 859] loss: 0.606476664543\n",
      "[step: 860] loss: 0.605731487274\n",
      "[step: 861] loss: 0.604987680912\n",
      "[step: 862] loss: 0.604245007038\n",
      "[step: 863] loss: 0.60350304842\n",
      "[step: 864] loss: 0.602762520313\n",
      "[step: 865] loss: 0.602022528648\n",
      "[step: 866] loss: 0.601283729076\n",
      "[step: 867] loss: 0.600545704365\n",
      "[step: 868] loss: 0.599808812141\n",
      "[step: 869] loss: 0.599072575569\n",
      "[step: 870] loss: 0.59833753109\n",
      "[step: 871] loss: 0.597603619099\n",
      "[step: 872] loss: 0.596869766712\n",
      "[step: 873] loss: 0.596137702465\n",
      "[step: 874] loss: 0.595406353474\n",
      "[step: 875] loss: 0.594675958157\n",
      "[step: 876] loss: 0.593946099281\n",
      "[step: 877] loss: 0.593217372894\n",
      "[step: 878] loss: 0.592489302158\n",
      "[step: 879] loss: 0.591762304306\n",
      "[step: 880] loss: 0.591036558151\n",
      "[step: 881] loss: 0.590310931206\n",
      "[step: 882] loss: 0.589586555958\n",
      "[step: 883] loss: 0.588863134384\n",
      "[step: 884] loss: 0.588140308857\n",
      "[step: 885] loss: 0.587418615818\n",
      "[step: 886] loss: 0.586697161198\n",
      "[step: 887] loss: 0.585977196693\n",
      "[step: 888] loss: 0.585257470608\n",
      "[step: 889] loss: 0.584539055824\n",
      "[step: 890] loss: 0.583821475506\n",
      "[step: 891] loss: 0.583104014397\n",
      "[step: 892] loss: 0.582388162613\n",
      "[step: 893] loss: 0.581672668457\n",
      "[step: 894] loss: 0.580958545208\n",
      "[step: 895] loss: 0.580244421959\n",
      "[step: 896] loss: 0.57953119278\n",
      "[step: 897] loss: 0.578819274902\n",
      "[step: 898] loss: 0.578107893467\n",
      "[step: 899] loss: 0.577397227287\n",
      "[step: 900] loss: 0.576687276363\n",
      "[step: 901] loss: 0.575977921486\n",
      "[step: 902] loss: 0.575269579887\n",
      "[step: 903] loss: 0.574561715126\n",
      "[step: 904] loss: 0.57385468483\n",
      "[step: 905] loss: 0.573148846626\n",
      "[step: 906] loss: 0.572442829609\n",
      "[step: 907] loss: 0.571738779545\n",
      "[step: 908] loss: 0.571034610271\n",
      "[step: 909] loss: 0.570331096649\n",
      "[step: 910] loss: 0.569628834724\n",
      "[step: 911] loss: 0.568926930428\n",
      "[step: 912] loss: 0.568225443363\n",
      "[step: 913] loss: 0.567525327206\n",
      "[step: 914] loss: 0.566825509071\n",
      "[step: 915] loss: 0.566126644611\n",
      "[step: 916] loss: 0.565428316593\n",
      "[step: 917] loss: 0.564730703831\n",
      "[step: 918] loss: 0.564033329487\n",
      "[step: 919] loss: 0.563337564468\n",
      "[step: 920] loss: 0.562641680241\n",
      "[step: 921] loss: 0.561947226524\n",
      "[step: 922] loss: 0.561252772808\n",
      "[step: 923] loss: 0.560558915138\n",
      "[step: 924] loss: 0.55986648798\n",
      "[step: 925] loss: 0.559174120426\n",
      "[step: 926] loss: 0.558482646942\n",
      "[step: 927] loss: 0.557791829109\n",
      "[step: 928] loss: 0.557101428509\n",
      "[step: 929] loss: 0.556412100792\n",
      "[step: 930] loss: 0.555723071098\n",
      "[step: 931] loss: 0.555034339428\n",
      "[step: 932] loss: 0.554347217083\n",
      "[step: 933] loss: 0.553660035133\n",
      "[step: 934] loss: 0.552974045277\n",
      "[step: 935] loss: 0.55228805542\n",
      "[step: 936] loss: 0.551603078842\n",
      "[step: 937] loss: 0.550918698311\n",
      "[step: 938] loss: 0.550235033035\n",
      "[step: 939] loss: 0.549551665783\n",
      "[step: 940] loss: 0.5488691926\n",
      "[step: 941] loss: 0.548186957836\n",
      "[step: 942] loss: 0.547506034374\n",
      "[step: 943] loss: 0.546825170517\n",
      "[step: 944] loss: 0.546145141125\n",
      "[step: 945] loss: 0.545466125011\n",
      "[step: 946] loss: 0.544787287712\n",
      "[step: 947] loss: 0.544108808041\n",
      "[step: 948] loss: 0.543431341648\n",
      "[step: 949] loss: 0.54275393486\n",
      "[step: 950] loss: 0.542077958584\n",
      "[step: 951] loss: 0.541402339935\n",
      "[step: 952] loss: 0.540727257729\n",
      "[step: 953] loss: 0.54005241394\n",
      "[step: 954] loss: 0.539378523827\n",
      "[step: 955] loss: 0.538704991341\n",
      "[step: 956] loss: 0.538032531738\n",
      "[step: 957] loss: 0.53736025095\n",
      "[step: 958] loss: 0.536689043045\n",
      "[step: 959] loss: 0.536017894745\n",
      "[step: 960] loss: 0.535347402096\n",
      "[step: 961] loss: 0.53467798233\n",
      "[step: 962] loss: 0.534008800983\n",
      "[step: 963] loss: 0.533339977264\n",
      "[step: 964] loss: 0.532672226429\n",
      "[step: 965] loss: 0.532005131245\n",
      "[step: 966] loss: 0.531337976456\n",
      "[step: 967] loss: 0.530671715736\n",
      "[step: 968] loss: 0.530006110668\n",
      "[step: 969] loss: 0.529341340065\n",
      "[step: 970] loss: 0.528676688671\n",
      "[step: 971] loss: 0.528012812138\n",
      "[step: 972] loss: 0.527349829674\n",
      "[step: 973] loss: 0.526686966419\n",
      "[step: 974] loss: 0.526025176048\n",
      "[step: 975] loss: 0.525363564491\n",
      "[step: 976] loss: 0.524702429771\n",
      "[step: 977] loss: 0.524042248726\n",
      "[step: 978] loss: 0.523382663727\n",
      "[step: 979] loss: 0.522723257542\n",
      "[step: 980] loss: 0.522064566612\n",
      "[step: 981] loss: 0.521406710148\n",
      "[step: 982] loss: 0.520749390125\n",
      "[step: 983] loss: 0.520092487335\n",
      "[step: 984] loss: 0.519436299801\n",
      "[step: 985] loss: 0.518781006336\n",
      "[step: 986] loss: 0.518126130104\n",
      "[step: 987] loss: 0.517471194267\n",
      "[step: 988] loss: 0.516817510128\n",
      "[step: 989] loss: 0.516164362431\n",
      "[step: 990] loss: 0.515511572361\n",
      "[step: 991] loss: 0.514859855175\n",
      "[step: 992] loss: 0.514208197594\n",
      "[step: 993] loss: 0.513557076454\n",
      "[step: 994] loss: 0.512906968594\n",
      "[step: 995] loss: 0.512256979942\n",
      "[step: 996] loss: 0.511608242989\n",
      "[step: 997] loss: 0.510959625244\n",
      "[step: 998] loss: 0.510311543941\n",
      "[step: 999] loss: 0.509664535522\n",
      "[[[0.72874494 0.72588832 0.73635427 0.03394607 0.73373984]\n",
      "  [0.72064777 0.72588832 0.72193615 0.03399039 0.72764228]\n",
      "  [0.72672065 0.72588832 0.73017508 0.03663479 0.72560976]\n",
      "  [0.69230769 0.72385787 0.70545829 0.08328343 0.73170732]\n",
      "  [0.69635628 0.69543147 0.68486097 0.0446695  0.69105691]\n",
      "  [0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]]]\n",
      "\n",
      "\n",
      "[[0.66391754 0.66874003 0.6650082  0.66741717 0.66782624]]\n",
      "[[[0.72064777 0.72588832 0.72193615 0.03399039 0.72764228]\n",
      "  [0.72672065 0.72588832 0.73017508 0.03663479 0.72560976]\n",
      "  [0.69230769 0.72385787 0.70545829 0.08328343 0.73170732]\n",
      "  [0.69635628 0.69543147 0.68486097 0.0446695  0.69105691]\n",
      "  [0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]]]\n",
      "\n",
      "\n",
      "[[0.6747469  0.66140145 0.65611273 0.6689201  0.6576984 ]]\n",
      "[[[0.72672065 0.72588832 0.73017508 0.03663479 0.72560976]\n",
      "  [0.69230769 0.72385787 0.70545829 0.08328343 0.73170732]\n",
      "  [0.69635628 0.69543147 0.68486097 0.0446695  0.69105691]\n",
      "  [0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]]]\n",
      "\n",
      "\n",
      "[[0.6838064 0.6705909 0.6676901 0.6750927 0.6679171]]\n",
      "[[[0.69230769 0.72385787 0.70545829 0.08328343 0.73170732]\n",
      "  [0.69635628 0.69543147 0.68486097 0.0446695  0.69105691]\n",
      "  [0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]]]\n",
      "\n",
      "\n",
      "[[0.6895872  0.681862   0.67848307 0.68222195 0.6782949 ]]\n",
      "[[[0.69635628 0.69543147 0.68486097 0.0446695  0.69105691]\n",
      "  [0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]]]\n",
      "\n",
      "\n",
      "[[0.69386935 0.6922301  0.68599975 0.6883881  0.6859691 ]]\n",
      "[[[0.66801619 0.69340102 0.68280124 0.09319577 0.69512195]\n",
      "  [0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]\n",
      "  [0.69386935 0.69223011 0.68599975 0.68838811 0.68596911]]]\n",
      "\n",
      "\n",
      "[[0.6985124  0.7020999  0.6927617  0.69501835 0.6932242 ]]\n",
      "[[[0.65384615 0.6751269  0.66838311 0.0588724  0.68292683]\n",
      "  [0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]\n",
      "  [0.69386935 0.69223011 0.68599975 0.68838811 0.68596911]\n",
      "  [0.69851238 0.70209992 0.69276172 0.69501835 0.69322419]]]\n",
      "\n",
      "\n",
      "[[0.7046818  0.7119969  0.70110816 0.70305794 0.7019936 ]]\n",
      "[[[0.66391754 0.66874003 0.66500819 0.66741717 0.66782624]\n",
      "  [0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]\n",
      "  [0.69386935 0.69223011 0.68599975 0.68838811 0.68596911]\n",
      "  [0.69851238 0.70209992 0.69276172 0.69501835 0.69322419]\n",
      "  [0.70468181 0.71199691 0.70110816 0.70305794 0.70199358]]]\n",
      "\n",
      "\n",
      "[[0.71382    0.7237959  0.71202695 0.71484965 0.71388644]]\n",
      "[[[0.67474687 0.66140145 0.65611273 0.6689201  0.65769839]\n",
      "  [0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]\n",
      "  [0.69386935 0.69223011 0.68599975 0.68838811 0.68596911]\n",
      "  [0.69851238 0.70209992 0.69276172 0.69501835 0.69322419]\n",
      "  [0.70468181 0.71199691 0.70110816 0.70305794 0.70199358]\n",
      "  [0.71381998 0.72379589 0.71202695 0.71484965 0.71388644]]]\n",
      "\n",
      "\n",
      "[[0.725335   0.73540497 0.7235455  0.7274683  0.7257332 ]]\n",
      "[[[0.68380642 0.67059088 0.6676901  0.6750927  0.66791707]\n",
      "  [0.68958718 0.681862   0.67848307 0.68222195 0.6782949 ]\n",
      "  [0.69386935 0.69223011 0.68599975 0.68838811 0.68596911]\n",
      "  [0.69851238 0.70209992 0.69276172 0.69501835 0.69322419]\n",
      "  [0.70468181 0.71199691 0.70110816 0.70305794 0.70199358]\n",
      "  [0.71381998 0.72379589 0.71202695 0.71484965 0.71388644]\n",
      "  [0.725335   0.73540497 0.72354549 0.72746831 0.72573322]]]\n",
      "\n",
      "\n",
      "[[0.7375218  0.7473476  0.73638976 0.74033445 0.7386769 ]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script shows how to predict stock prices using a basic RNN\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "tf.set_random_seed(777) # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 5\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./data/inputs.csv', delimiter=',')\n",
    "xy = xy[::-1] # reverse order (chronically ordered)\n",
    "xy = MinMaxScaler(xy)\n",
    "x = xy\n",
    "y = xy[:, [-1]] # Close as label\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length] # Next close price\n",
    "    #print(_x, \"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None) # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y)) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "    inputX = testX[0:1,:,:]\n",
    "    \n",
    "    from numpy import newaxis\n",
    "\n",
    "    testY_len = 10\n",
    "    testY_hat = np.zeros((testY_len, 1))\n",
    "    \n",
    "    for i in range(testY_len):\n",
    "        test_predict = sess.run(Y_pred, feed_dict={X: inputX})\n",
    "        print inputX\n",
    "        print '\\n'\n",
    "\n",
    "        inputX = inputX[:,1:,:]\n",
    "        inputX = np.append(inputX, test_predict[newaxis, :, :], axis=1)\n",
    "        \n",
    "        testY_hat[i,:] = float(test_predict[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b9c623b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Plot predictions\n",
    "    plt.plot(testY[:testY_len,:])\n",
    "    plt.plot(testY_hat)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66782624]\n",
      " [0.65769839]\n",
      " [0.66791707]\n",
      " [0.6782949 ]\n",
      " [0.68596911]\n",
      " [0.69322419]\n",
      " [0.70199358]\n",
      " [0.71388644]\n",
      " [0.72573322]\n",
      " [0.73867691]]\n"
     ]
    }
   ],
   "source": [
    "print testY_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "\n",
    "    # Successful Rate\n",
    "    sum_score = 0.0;\n",
    "\n",
    "    for i in range(0, len(test_predict)-1):\n",
    "        if (test_predict[i,-1] < testY[i+1,-1]) == (test_predict[i,-1] < test_predict[i+1,-1]):\n",
    "            sum_score = sum_score + 1\n",
    "\n",
    "    sum_score = sum_score/(len(test_predict)-1)\n",
    "\n",
    "    print sum_score\n",
    "\n",
    "    '''\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: testY, predictions: test_predict})\n",
    "\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    '''\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(testY)\n",
    "    plt.plot(test_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
