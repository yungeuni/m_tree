{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 551.856506348\n",
      "[step: 1] loss: 402.725799561\n",
      "[step: 2] loss: 278.913604736\n",
      "[step: 3] loss: 182.313873291\n",
      "[step: 4] loss: 117.207847595\n",
      "[step: 5] loss: 84.9883499146\n",
      "[step: 6] loss: 81.1049194336\n",
      "[step: 7] loss: 92.1679153442\n",
      "[step: 8] loss: 96.8047714233\n",
      "[step: 9] loss: 88.0680465698\n",
      "[step: 10] loss: 71.4161148071\n",
      "[step: 11] loss: 53.8768424988\n",
      "[step: 12] loss: 40.0232467651\n",
      "[step: 13] loss: 31.5744132996\n",
      "[step: 14] loss: 28.1536693573\n",
      "[step: 15] loss: 28.2720088959\n",
      "[step: 16] loss: 30.1617546082\n",
      "[step: 17] loss: 32.2988014221\n",
      "[step: 18] loss: 33.6187973022\n",
      "[step: 19] loss: 33.5376205444\n",
      "[step: 20] loss: 31.8935680389\n",
      "[step: 21] loss: 28.8724956512\n",
      "[step: 22] loss: 24.9196300507\n",
      "[step: 23] loss: 20.621963501\n",
      "[step: 24] loss: 16.5641555786\n",
      "[step: 25] loss: 13.1928386688\n",
      "[step: 26] loss: 10.7368345261\n",
      "[step: 27] loss: 9.20631599426\n",
      "[step: 28] loss: 8.45013046265\n",
      "[step: 29] loss: 8.22612476349\n",
      "[step: 30] loss: 8.2575712204\n",
      "[step: 31] loss: 8.28377056122\n",
      "[step: 32] loss: 8.11844444275\n",
      "[step: 33] loss: 7.70085954666\n",
      "[step: 34] loss: 7.10480642319\n",
      "[step: 35] loss: 6.49039554596\n",
      "[step: 36] loss: 6.0208106041\n",
      "[step: 37] loss: 5.78644561768\n",
      "[step: 38] loss: 5.77195835114\n",
      "[step: 39] loss: 5.876932621\n",
      "[step: 40] loss: 5.97263622284\n",
      "[step: 41] loss: 5.96104383469\n",
      "[step: 42] loss: 5.80788421631\n",
      "[step: 43] loss: 5.54218053818\n",
      "[step: 44] loss: 5.23292303085\n",
      "[step: 45] loss: 4.95828294754\n",
      "[step: 46] loss: 4.77798986435\n",
      "[step: 47] loss: 4.71440553665\n",
      "[step: 48] loss: 4.74667930603\n",
      "[step: 49] loss: 4.82094669342\n",
      "[step: 50] loss: 4.87417840958\n",
      "[step: 51] loss: 4.86132335663\n",
      "[step: 52] loss: 4.77210855484\n",
      "[step: 53] loss: 4.62944889069\n",
      "[step: 54] loss: 4.47262334824\n",
      "[step: 55] loss: 4.33683586121\n",
      "[step: 56] loss: 4.24020004272\n",
      "[step: 57] loss: 4.18212747574\n",
      "[step: 58] loss: 4.14970874786\n",
      "[step: 59] loss: 4.12626791\n",
      "[step: 60] loss: 4.09819221497\n",
      "[step: 61] loss: 4.05893230438\n",
      "[step: 62] loss: 4.01031827927\n",
      "[step: 63] loss: 3.96090841293\n",
      "[step: 64] loss: 3.92136573792\n",
      "[step: 65] loss: 3.89858937263\n",
      "[step: 66] loss: 3.89181041718\n",
      "[step: 67] loss: 3.89320874214\n",
      "[step: 68] loss: 3.89247655869\n",
      "[step: 69] loss: 3.88220930099\n",
      "[step: 70] loss: 3.86077618599\n",
      "[step: 71] loss: 3.83167886734\n",
      "[step: 72] loss: 3.80089068413\n",
      "[step: 73] loss: 3.77415847778\n",
      "[step: 74] loss: 3.75517392159\n",
      "[step: 75] loss: 3.74468636513\n",
      "[step: 76] loss: 3.74039459229\n",
      "[step: 77] loss: 3.73790454865\n",
      "[step: 78] loss: 3.73265075684\n",
      "[step: 79] loss: 3.72187995911\n",
      "[step: 80] loss: 3.70564675331\n",
      "[step: 81] loss: 3.68619227409\n",
      "[step: 82] loss: 3.66638946533\n",
      "[step: 83] loss: 3.64833235741\n",
      "[step: 84] loss: 3.6327662468\n",
      "[step: 85] loss: 3.61931085587\n",
      "[step: 86] loss: 3.60700392723\n",
      "[step: 87] loss: 3.59480905533\n",
      "[step: 88] loss: 3.58207011223\n",
      "[step: 89] loss: 3.56876778603\n",
      "[step: 90] loss: 3.5554766655\n",
      "[step: 91] loss: 3.54292988777\n",
      "[step: 92] loss: 3.53146791458\n",
      "[step: 93] loss: 3.52078771591\n",
      "[step: 94] loss: 3.5101788044\n",
      "[step: 95] loss: 3.49901366234\n",
      "[step: 96] loss: 3.48706912994\n",
      "[step: 97] loss: 3.4745824337\n",
      "[step: 98] loss: 3.46201753616\n",
      "[step: 99] loss: 3.44985699654\n",
      "[step: 100] loss: 3.438403368\n",
      "[step: 101] loss: 3.42766308784\n",
      "[step: 102] loss: 3.41733026505\n",
      "[step: 103] loss: 3.40693902969\n",
      "[step: 104] loss: 3.39611673355\n",
      "[step: 105] loss: 3.38476204872\n",
      "[step: 106] loss: 3.37304830551\n",
      "[step: 107] loss: 3.36126112938\n",
      "[step: 108] loss: 3.34961771965\n",
      "[step: 109] loss: 3.33821582794\n",
      "[step: 110] loss: 3.32700943947\n",
      "[step: 111] loss: 3.31589531898\n",
      "[step: 112] loss: 3.30474925041\n",
      "[step: 113] loss: 3.29350280762\n",
      "[step: 114] loss: 3.28218388557\n",
      "[step: 115] loss: 3.27087306976\n",
      "[step: 116] loss: 3.25965046883\n",
      "[step: 117] loss: 3.24852490425\n",
      "[step: 118] loss: 3.23744821548\n",
      "[step: 119] loss: 3.2263507843\n",
      "[step: 120] loss: 3.21519589424\n",
      "[step: 121] loss: 3.20398449898\n",
      "[step: 122] loss: 3.19274997711\n",
      "[step: 123] loss: 3.18154025078\n",
      "[step: 124] loss: 3.17039203644\n",
      "[step: 125] loss: 3.15931296349\n",
      "[step: 126] loss: 3.14827513695\n",
      "[step: 127] loss: 3.13723707199\n",
      "[step: 128] loss: 3.12616658211\n",
      "[step: 129] loss: 3.11505889893\n",
      "[step: 130] loss: 3.103931427\n",
      "[step: 131] loss: 3.09280943871\n",
      "[step: 132] loss: 3.08170747757\n",
      "[step: 133] loss: 3.07062506676\n",
      "[step: 134] loss: 3.05955934525\n",
      "[step: 135] loss: 3.04849433899\n",
      "[step: 136] loss: 3.03742694855\n",
      "[step: 137] loss: 3.02635741234\n",
      "[step: 138] loss: 3.01529836655\n",
      "[step: 139] loss: 3.00426077843\n",
      "[step: 140] loss: 2.99324297905\n",
      "[step: 141] loss: 2.98224401474\n",
      "[step: 142] loss: 2.97125387192\n",
      "[step: 143] loss: 2.96026420593\n",
      "[step: 144] loss: 2.94927763939\n",
      "[step: 145] loss: 2.93829393387\n",
      "[step: 146] loss: 2.92732429504\n",
      "[step: 147] loss: 2.91637134552\n",
      "[step: 148] loss: 2.90543532372\n",
      "[step: 149] loss: 2.89451694489\n",
      "[step: 150] loss: 2.8836081028\n",
      "[step: 151] loss: 2.87271213531\n",
      "[step: 152] loss: 2.86182260513\n",
      "[step: 153] loss: 2.85094618797\n",
      "[step: 154] loss: 2.84008526802\n",
      "[step: 155] loss: 2.82923698425\n",
      "[step: 156] loss: 2.81840920448\n",
      "[step: 157] loss: 2.80759692192\n",
      "[step: 158] loss: 2.79679775238\n",
      "[step: 159] loss: 2.78601312637\n",
      "[step: 160] loss: 2.77524471283\n",
      "[step: 161] loss: 2.76449370384\n",
      "[step: 162] loss: 2.75376200676\n",
      "[step: 163] loss: 2.7430460453\n",
      "[step: 164] loss: 2.73234891891\n",
      "[step: 165] loss: 2.72167134285\n",
      "[step: 166] loss: 2.7110106945\n",
      "[step: 167] loss: 2.70037007332\n",
      "[step: 168] loss: 2.68974876404\n",
      "[step: 169] loss: 2.67914819717\n",
      "[step: 170] loss: 2.66856718063\n",
      "[step: 171] loss: 2.65801119804\n",
      "[step: 172] loss: 2.64747309685\n",
      "[step: 173] loss: 2.63695597649\n",
      "[step: 174] loss: 2.62645959854\n",
      "[step: 175] loss: 2.61598372459\n",
      "[step: 176] loss: 2.6055328846\n",
      "[step: 177] loss: 2.59510374069\n",
      "[step: 178] loss: 2.58469724655\n",
      "[step: 179] loss: 2.57431387901\n",
      "[step: 180] loss: 2.56395864487\n",
      "[step: 181] loss: 2.5536236763\n",
      "[step: 182] loss: 2.54331445694\n",
      "[step: 183] loss: 2.53303074837\n",
      "[step: 184] loss: 2.52277231216\n",
      "[step: 185] loss: 2.51254177094\n",
      "[step: 186] loss: 2.50233674049\n",
      "[step: 187] loss: 2.49215769768\n",
      "[step: 188] loss: 2.482006073\n",
      "[step: 189] loss: 2.47188091278\n",
      "[step: 190] loss: 2.4617857933\n",
      "[step: 191] loss: 2.45171785355\n",
      "[step: 192] loss: 2.44167971611\n",
      "[step: 193] loss: 2.43166923523\n",
      "[step: 194] loss: 2.42169141769\n",
      "[step: 195] loss: 2.41174077988\n",
      "[step: 196] loss: 2.40182113647\n",
      "[step: 197] loss: 2.39193320274\n",
      "[step: 198] loss: 2.38207650185\n",
      "[step: 199] loss: 2.37225079536\n",
      "[step: 200] loss: 2.36245894432\n",
      "[step: 201] loss: 2.35269832611\n",
      "[step: 202] loss: 2.34296870232\n",
      "[step: 203] loss: 2.33327412605\n",
      "[step: 204] loss: 2.32361268997\n",
      "[step: 205] loss: 2.31398630142\n",
      "[step: 206] loss: 2.30439543724\n",
      "[step: 207] loss: 2.2948372364\n",
      "[step: 208] loss: 2.28531336784\n",
      "[step: 209] loss: 2.27582764626\n",
      "[step: 210] loss: 2.26637578011\n",
      "[step: 211] loss: 2.256960392\n",
      "[step: 212] loss: 2.24758315086\n",
      "[step: 213] loss: 2.23824357986\n",
      "[step: 214] loss: 2.22894024849\n",
      "[step: 215] loss: 2.21967387199\n",
      "[step: 216] loss: 2.21044683456\n",
      "[step: 217] loss: 2.20125865936\n",
      "[step: 218] loss: 2.19210839272\n",
      "[step: 219] loss: 2.1829996109\n",
      "[step: 220] loss: 2.17392921448\n",
      "[step: 221] loss: 2.16489911079\n",
      "[step: 222] loss: 2.15590882301\n",
      "[step: 223] loss: 2.1469604969\n",
      "[step: 224] loss: 2.13805246353\n",
      "[step: 225] loss: 2.1291847229\n",
      "[step: 226] loss: 2.12036108971\n",
      "[step: 227] loss: 2.11157846451\n",
      "[step: 228] loss: 2.10283780098\n",
      "[step: 229] loss: 2.09413981438\n",
      "[step: 230] loss: 2.08548593521\n",
      "[step: 231] loss: 2.07687473297\n",
      "[step: 232] loss: 2.06830573082\n",
      "[step: 233] loss: 2.05978345871\n",
      "[step: 234] loss: 2.05130457878\n",
      "[step: 235] loss: 2.04287004471\n",
      "[step: 236] loss: 2.03447985649\n",
      "[step: 237] loss: 2.02613353729\n",
      "[step: 238] loss: 2.01783585548\n",
      "[step: 239] loss: 2.00958180428\n",
      "[step: 240] loss: 2.00137329102\n",
      "[step: 241] loss: 1.99321103096\n",
      "[step: 242] loss: 1.98509609699\n",
      "[step: 243] loss: 1.97702634335\n",
      "[step: 244] loss: 1.96900451183\n",
      "[step: 245] loss: 1.96103000641\n",
      "[step: 246] loss: 1.95310258865\n",
      "[step: 247] loss: 1.94522213936\n",
      "[step: 248] loss: 1.93738746643\n",
      "[step: 249] loss: 1.92960309982\n",
      "[step: 250] loss: 1.92186689377\n",
      "[step: 251] loss: 1.91417765617\n",
      "[step: 252] loss: 1.90653824806\n",
      "[step: 253] loss: 1.89894533157\n",
      "[step: 254] loss: 1.89140272141\n",
      "[step: 255] loss: 1.88390731812\n",
      "[step: 256] loss: 1.87646138668\n",
      "[step: 257] loss: 1.8690649271\n",
      "[step: 258] loss: 1.86171770096\n",
      "[step: 259] loss: 1.85441923141\n",
      "[step: 260] loss: 1.84717047215\n",
      "[step: 261] loss: 1.83997070789\n",
      "[step: 262] loss: 1.83282053471\n",
      "[step: 263] loss: 1.82572054863\n",
      "[step: 264] loss: 1.81866979599\n",
      "[step: 265] loss: 1.81166851521\n",
      "[step: 266] loss: 1.80471742153\n",
      "[step: 267] loss: 1.79781496525\n",
      "[step: 268] loss: 1.7909642458\n",
      "[step: 269] loss: 1.78416097164\n",
      "[step: 270] loss: 1.77740955353\n",
      "[step: 271] loss: 1.77070701122\n",
      "[step: 272] loss: 1.76405405998\n",
      "[step: 273] loss: 1.7574505806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 274] loss: 1.750898242\n",
      "[step: 275] loss: 1.74439406395\n",
      "[step: 276] loss: 1.73793935776\n",
      "[step: 277] loss: 1.73153603077\n",
      "[step: 278] loss: 1.72517943382\n",
      "[step: 279] loss: 1.71887457371\n",
      "[step: 280] loss: 1.71261715889\n",
      "[step: 281] loss: 1.70640969276\n",
      "[step: 282] loss: 1.70025193691\n",
      "[step: 283] loss: 1.69414186478\n",
      "[step: 284] loss: 1.68808197975\n",
      "[step: 285] loss: 1.68206882477\n",
      "[step: 286] loss: 1.67610633373\n",
      "[step: 287] loss: 1.67019128799\n",
      "[step: 288] loss: 1.66432368755\n",
      "[step: 289] loss: 1.65850448608\n",
      "[step: 290] loss: 1.65273332596\n",
      "[step: 291] loss: 1.64701008797\n",
      "[step: 292] loss: 1.64133310318\n",
      "[step: 293] loss: 1.63570380211\n",
      "[step: 294] loss: 1.63012135029\n",
      "[step: 295] loss: 1.62458598614\n",
      "[step: 296] loss: 1.61909687519\n",
      "[step: 297] loss: 1.61365377903\n",
      "[step: 298] loss: 1.60825705528\n",
      "[step: 299] loss: 1.60290491581\n",
      "[step: 300] loss: 1.59759879112\n",
      "[step: 301] loss: 1.59233808517\n",
      "[step: 302] loss: 1.58712100983\n",
      "[step: 303] loss: 1.58195030689\n",
      "[step: 304] loss: 1.57682287693\n",
      "[step: 305] loss: 1.57173860073\n",
      "[step: 306] loss: 1.56669890881\n",
      "[step: 307] loss: 1.56170237064\n",
      "[step: 308] loss: 1.55674803257\n",
      "[step: 309] loss: 1.55183649063\n",
      "[step: 310] loss: 1.54696750641\n",
      "[step: 311] loss: 1.54214036465\n",
      "[step: 312] loss: 1.537353158\n",
      "[step: 313] loss: 1.53260827065\n",
      "[step: 314] loss: 1.52790534496\n",
      "[step: 315] loss: 1.52324044704\n",
      "[step: 316] loss: 1.5186175108\n",
      "[step: 317] loss: 1.51403295994\n",
      "[step: 318] loss: 1.50948822498\n",
      "[step: 319] loss: 1.50498211384\n",
      "[step: 320] loss: 1.50051391125\n",
      "[step: 321] loss: 1.49608385563\n",
      "[step: 322] loss: 1.49169242382\n",
      "[step: 323] loss: 1.48733794689\n",
      "[step: 324] loss: 1.48301923275\n",
      "[step: 325] loss: 1.47873747349\n",
      "[step: 326] loss: 1.47449195385\n",
      "[step: 327] loss: 1.47028124332\n",
      "[step: 328] loss: 1.46610617638\n",
      "[step: 329] loss: 1.46196591854\n",
      "[step: 330] loss: 1.45786046982\n",
      "[step: 331] loss: 1.4537872076\n",
      "[step: 332] loss: 1.44974911213\n",
      "[step: 333] loss: 1.44574296474\n",
      "[step: 334] loss: 1.44177031517\n",
      "[step: 335] loss: 1.4378298521\n",
      "[step: 336] loss: 1.43392014503\n",
      "[step: 337] loss: 1.4300429821\n",
      "[step: 338] loss: 1.42619621754\n",
      "[step: 339] loss: 1.42237961292\n",
      "[step: 340] loss: 1.41859376431\n",
      "[step: 341] loss: 1.41483712196\n",
      "[step: 342] loss: 1.41111040115\n",
      "[step: 343] loss: 1.40741205215\n",
      "[step: 344] loss: 1.40374267101\n",
      "[step: 345] loss: 1.4001005888\n",
      "[step: 346] loss: 1.39648795128\n",
      "[step: 347] loss: 1.39289975166\n",
      "[step: 348] loss: 1.38934099674\n",
      "[step: 349] loss: 1.38580691814\n",
      "[step: 350] loss: 1.38229978085\n",
      "[step: 351] loss: 1.37881827354\n",
      "[step: 352] loss: 1.37536180019\n",
      "[step: 353] loss: 1.37193083763\n",
      "[step: 354] loss: 1.36852395535\n",
      "[step: 355] loss: 1.36514186859\n",
      "[step: 356] loss: 1.36178302765\n",
      "[step: 357] loss: 1.35844755173\n",
      "[step: 358] loss: 1.35513663292\n",
      "[step: 359] loss: 1.35184645653\n",
      "[step: 360] loss: 1.34857988358\n",
      "[step: 361] loss: 1.34533441067\n",
      "[step: 362] loss: 1.34211170673\n",
      "[step: 363] loss: 1.33891010284\n",
      "[step: 364] loss: 1.33572924137\n",
      "[step: 365] loss: 1.33257007599\n",
      "[step: 366] loss: 1.32942950726\n",
      "[step: 367] loss: 1.32631075382\n",
      "[step: 368] loss: 1.32321119308\n",
      "[step: 369] loss: 1.32013070583\n",
      "[step: 370] loss: 1.31706976891\n",
      "[step: 371] loss: 1.3140283823\n",
      "[step: 372] loss: 1.31100451946\n",
      "[step: 373] loss: 1.30799901485\n",
      "[step: 374] loss: 1.30501234531\n",
      "[step: 375] loss: 1.30204355717\n",
      "[step: 376] loss: 1.29909241199\n",
      "[step: 377] loss: 1.29615664482\n",
      "[step: 378] loss: 1.29323899746\n",
      "[step: 379] loss: 1.29033851624\n",
      "[step: 380] loss: 1.28745496273\n",
      "[step: 381] loss: 1.28458607197\n",
      "[step: 382] loss: 1.28173387051\n",
      "[step: 383] loss: 1.27889764309\n",
      "[step: 384] loss: 1.2760771513\n",
      "[step: 385] loss: 1.27327108383\n",
      "[step: 386] loss: 1.27048122883\n",
      "[step: 387] loss: 1.26770567894\n",
      "[step: 388] loss: 1.26494467258\n",
      "[step: 389] loss: 1.26219797134\n",
      "[step: 390] loss: 1.25946581364\n",
      "[step: 391] loss: 1.25674772263\n",
      "[step: 392] loss: 1.25404298306\n",
      "[step: 393] loss: 1.25135219097\n",
      "[step: 394] loss: 1.24867510796\n",
      "[step: 395] loss: 1.24601125717\n",
      "[step: 396] loss: 1.24336028099\n",
      "[step: 397] loss: 1.24072265625\n",
      "[step: 398] loss: 1.23809742928\n",
      "[step: 399] loss: 1.23548483849\n",
      "[step: 400] loss: 1.2328851223\n",
      "[step: 401] loss: 1.23029696941\n",
      "[step: 402] loss: 1.22772121429\n",
      "[step: 403] loss: 1.22515809536\n",
      "[step: 404] loss: 1.22260570526\n",
      "[step: 405] loss: 1.2200666666\n",
      "[step: 406] loss: 1.21753752232\n",
      "[step: 407] loss: 1.21501994133\n",
      "[step: 408] loss: 1.21251416206\n",
      "[step: 409] loss: 1.21001982689\n",
      "[step: 410] loss: 1.20753633976\n",
      "[step: 411] loss: 1.20506298542\n",
      "[step: 412] loss: 1.20260107517\n",
      "[step: 413] loss: 1.20015025139\n",
      "[step: 414] loss: 1.19770884514\n",
      "[step: 415] loss: 1.19527876377\n",
      "[step: 416] loss: 1.19285869598\n",
      "[step: 417] loss: 1.19044864178\n",
      "[step: 418] loss: 1.18804979324\n",
      "[step: 419] loss: 1.18565928936\n",
      "[step: 420] loss: 1.18327975273\n",
      "[step: 421] loss: 1.18090987206\n",
      "[step: 422] loss: 1.17854893208\n",
      "[step: 423] loss: 1.1761983633\n",
      "[step: 424] loss: 1.17385697365\n",
      "[step: 425] loss: 1.17152512074\n",
      "[step: 426] loss: 1.16920280457\n",
      "[step: 427] loss: 1.16688907146\n",
      "[step: 428] loss: 1.16458511353\n",
      "[step: 429] loss: 1.16229057312\n",
      "[step: 430] loss: 1.16000497341\n",
      "[step: 431] loss: 1.15772747993\n",
      "[step: 432] loss: 1.15545904636\n",
      "[step: 433] loss: 1.15319907665\n",
      "[step: 434] loss: 1.15094828606\n",
      "[step: 435] loss: 1.14870595932\n",
      "[step: 436] loss: 1.1464728117\n",
      "[step: 437] loss: 1.14424741268\n",
      "[step: 438] loss: 1.14203023911\n",
      "[step: 439] loss: 1.13982176781\n",
      "[step: 440] loss: 1.13762152195\n",
      "[step: 441] loss: 1.13542973995\n",
      "[step: 442] loss: 1.13324546814\n",
      "[step: 443] loss: 1.13106918335\n",
      "[step: 444] loss: 1.12890124321\n",
      "[step: 445] loss: 1.12674081326\n",
      "[step: 446] loss: 1.12458944321\n",
      "[step: 447] loss: 1.12244451046\n",
      "[step: 448] loss: 1.12030827999\n",
      "[step: 449] loss: 1.11817991734\n",
      "[step: 450] loss: 1.1160582304\n",
      "[step: 451] loss: 1.1139446497\n",
      "[step: 452] loss: 1.11183953285\n",
      "[step: 453] loss: 1.10973966122\n",
      "[step: 454] loss: 1.10764861107\n",
      "[step: 455] loss: 1.10556447506\n",
      "[step: 456] loss: 1.10348796844\n",
      "[step: 457] loss: 1.1014187336\n",
      "[step: 458] loss: 1.09935760498\n",
      "[step: 459] loss: 1.0973020792\n",
      "[step: 460] loss: 1.09525430202\n",
      "[step: 461] loss: 1.0932135582\n",
      "[step: 462] loss: 1.09118044376\n",
      "[step: 463] loss: 1.08915364742\n",
      "[step: 464] loss: 1.08713388443\n",
      "[step: 465] loss: 1.08512091637\n",
      "[step: 466] loss: 1.08311522007\n",
      "[step: 467] loss: 1.08111655712\n",
      "[step: 468] loss: 1.07912409306\n",
      "[step: 469] loss: 1.07713770866\n",
      "[step: 470] loss: 1.07515883446\n",
      "[step: 471] loss: 1.07318627834\n",
      "[step: 472] loss: 1.07122063637\n",
      "[step: 473] loss: 1.0692615509\n",
      "[step: 474] loss: 1.06730926037\n",
      "[step: 475] loss: 1.0653629303\n",
      "[step: 476] loss: 1.06342291832\n",
      "[step: 477] loss: 1.06148946285\n",
      "[step: 478] loss: 1.05956268311\n",
      "[step: 479] loss: 1.0576416254\n",
      "[step: 480] loss: 1.05572724342\n",
      "[step: 481] loss: 1.05381894112\n",
      "[step: 482] loss: 1.05191671848\n",
      "[step: 483] loss: 1.05002081394\n",
      "[step: 484] loss: 1.04813146591\n",
      "[step: 485] loss: 1.04624772072\n",
      "[step: 486] loss: 1.04437017441\n",
      "[step: 487] loss: 1.04249894619\n",
      "[step: 488] loss: 1.04063355923\n",
      "[step: 489] loss: 1.03877365589\n",
      "[step: 490] loss: 1.03692007065\n",
      "[step: 491] loss: 1.03507232666\n",
      "[step: 492] loss: 1.0332300663\n",
      "[step: 493] loss: 1.03139412403\n",
      "[step: 494] loss: 1.02956414223\n",
      "[step: 495] loss: 1.02773940563\n",
      "[step: 496] loss: 1.0259206295\n",
      "[step: 497] loss: 1.024107337\n",
      "[step: 498] loss: 1.02229988575\n",
      "[step: 499] loss: 1.02049815655\n",
      "[[0.6686667  0.66996706 0.6741871  0.6658988  0.6734777 ]]\n",
      "[[0.6746378 0.6675511 0.6548299 0.6702177 0.6564601]]\n",
      "[[0.6779463 0.6729377 0.6590465 0.6702345 0.6594608]]\n",
      "[[0.682734   0.68206453 0.6688762  0.67348796 0.66777134]]\n",
      "[[0.6894759  0.69337344 0.6805838  0.67945856 0.67806095]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script shows how to predict stock prices using a basic RNN\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "tf.set_random_seed(777) # reproducibility\n",
    "\n",
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "# train Parameters\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 5\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "\n",
    "# Open, High, Low, Volume, Close\n",
    "xy = np.loadtxt('./data/inputs.csv', delimiter=',')\n",
    "xy = xy[::-1] # reverse order (chronically ordered)\n",
    "xy = MinMaxScaler(xy)\n",
    "x = xy\n",
    "y = xy[:, [-1]] # Close as label\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length] # Next close price\n",
    "    #print(_x, \"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "# train/test split\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None) # We use the last cell's output\n",
    "\n",
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y)) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "    inputX = testX[0:1,:,:]\n",
    "    \n",
    "    from numpy import newaxis\n",
    "\n",
    "    testY_hat = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        test_predict = sess.run(Y_pred, feed_dict={X: inputX})\n",
    "        print test_predict\n",
    "\n",
    "        inputX = inputX[:,1:,:]\n",
    "        inputX = np.append(inputX, test_predict[newaxis, :, :], axis=1)\n",
    "        \n",
    "        testY_hat = [testY_hat, test_predict];\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY_hat = [testY_hat, test_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[], array([0.67806095], dtype=float32)],\n",
       "  array([[0.6894759 , 0.69337344, 0.6805838 , 0.67945856, 0.67806095]],\n",
       "        dtype=float32)],\n",
       " array([[0.6894759 , 0.69337344, 0.6805838 , 0.67945856, 0.67806095]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    \n",
    "\n",
    "    # Successful Rate\n",
    "    sum_score = 0.0;\n",
    "\n",
    "    for i in range(0, len(test_predict)-1):\n",
    "        if (test_predict[i,-1] < testY[i+1,-1]) == (test_predict[i,-1] < test_predict[i+1,-1]):\n",
    "            sum_score = sum_score + 1\n",
    "\n",
    "    sum_score = sum_score/(len(test_predict)-1)\n",
    "\n",
    "    print sum_score\n",
    "\n",
    "    '''\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: testY, predictions: test_predict})\n",
    "\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "    '''\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(testY)\n",
    "    plt.plot(test_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
